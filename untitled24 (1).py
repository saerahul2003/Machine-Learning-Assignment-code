# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g5gfEbmtKi0hswd9BHIirqIBzW-zNNnj
"""



"""Sai Rahul CSE B 3122215001090 ML CAT1 Assignment 1

Consider the given MNIST dataset and apply Implement the Multi-Layer Perceptron
to classify the Handwritten characters. [CO2, K3] [1.1.1, 1.4.1, 2.3.1]
MNIST is a collection of handwritten digits ranging from the number 0 to 9.
It has a training set of 60,000 images, and 10,000 test images that are classified into
corresponding categories or labels.

Loads the MNIST dataset.
Normalizes the input images to have pixel values in the range [0, 1].
Flattens the images from 28x28 to a 1D array of length 784.
Converts the labels to one-hot encoded vectors.
Builds an MLP model using Keras with three dense layers.
Compiles the model with an Adam optimizer and categorical cross-entropy loss.
Trains the model on the training data for 5 epochs with a batch size of 32.
Evaluates the model on the test data and prints the test loss and accuracy.
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the input images
x_train = x_train / 255.0
x_test = x_test / 255.0

# Flatten the images
x_train_flatten = x_train.reshape(-1, 28*28)
x_test_flatten = x_test.reshape(-1, 28*28)

# Convert labels to one-hot encoded vectors
y_train_one_hot = to_categorical(y_train, num_classes=10)
y_test_one_hot = to_categorical(y_test, num_classes=10)

# Build the MLP model
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(x_train, y_train_one_hot, epochs=5, batch_size=32, validation_split=0.1)

# Evaluate the model on test set
test_loss, test_accuracy = model.evaluate(x_test, y_test_one_hot)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

# Print training accuracy and loss
train_accuracy = history.history['accuracy']
train_loss = history.history['loss']
print("Train Accuracy:", train_accuracy)
print("Train Loss:", train_loss)

# Plot training and validation accuracy values
plt.plot(train_accuracy, label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot training and validation loss values
plt.plot(train_loss, label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""RESULTS AFTER DOING DIMENSIONALITY REDUCTION (PCA)"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the input images
x_train = x_train / 255.0
x_test = x_test / 255.0

# Flatten the images
x_train_flatten = x_train.reshape(-1, 28*28)
x_test_flatten = x_test.reshape(-1, 28*28)

# Convert labels to one-hot encoded vectors
y_train_one_hot = to_categorical(y_train, num_classes=10)
y_test_one_hot = to_categorical(y_test, num_classes=10)

# Apply PCA for dimensionality reduction
pca = PCA(n_components=0.95)  # Retain 95% of variance
x_train_pca = pca.fit_transform(x_train_flatten)
x_test_pca = pca.transform(x_test_flatten)

# Build the MLP model
model = Sequential([
    Dense(128, activation='relu', input_shape=(x_train_pca.shape[1],)),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model with learning rate scheduling
initial_learning_rate = 0.01
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True
)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)

model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(x_train_pca, y_train_one_hot, epochs=5, batch_size=32, validation_split=0.1)

# Evaluate the model on test set
test_loss, test_accuracy = model.evaluate(x_test_pca, y_test_one_hot)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

# Print training accuracy and loss
train_accuracy = history.history['accuracy']
train_loss = history.history['loss']
print("Train Accuracy:", train_accuracy)
print("Train Loss:", train_loss)

# Plot training and validation accuracy values
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot training and validation loss values
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""RESULTS AFTER DIMENSIONALITY REDUCTION(LDA)"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import matplotlib.pyplot as plt

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the input images
x_train = x_train / 255.0
x_test = x_test / 255.0

# Flatten the images
x_train_flatten = x_train.reshape(-1, 28*28)
x_test_flatten = x_test.reshape(-1, 28*28)

# Convert labels to one-hot encoded vectors
y_train_one_hot = to_categorical(y_train, num_classes=10)
y_test_one_hot = to_categorical(y_test, num_classes=10)

# Apply LDA for dimensionality reduction
lda = LinearDiscriminantAnalysis(n_components=9)  # Reduced to 9 components
x_train_lda = lda.fit_transform(x_train_flatten, y_train)
x_test_lda = lda.transform(x_test_flatten)

# Build the MLP model
model = Sequential([
    Dense(128, activation='relu', input_shape=(x_train_lda.shape[1],)),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model with learning rate scheduling
initial_learning_rate = 0.01
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True
)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)

model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(x_train_lda, y_train_one_hot, epochs=5, batch_size=32, validation_split=0.1)

# Evaluate the model on test set
test_loss, test_accuracy = model.evaluate(x_test_lda, y_test_one_hot)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

# Print training accuracy and loss
train_accuracy = history.history['accuracy']
train_loss = history.history['loss']
print("Train Accuracy:", train_accuracy)
print("Train Loss:", train_loss)

# Plot training and validation accuracy values
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot training and validation loss values
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""Result: MNIST performs more well in MLP without dimensionality reduction (PCA,LDA)"""